# Epochs, hidden_size are for both translate.py and train.py
epochs: 200
batch_size: 32
# model can be 'ct', 'cat' or 'mpctm'
model: 'cat'
device_id: 0
random_seed: 31415
embedding_dim: 300
learning_rate: 0.0005
hidden_size: 200
dense_dim: 200
output_dim: 200
num_layers_lstm: 2
use_cuda: True
use_softmax_classifier: False
use_bin: False
use_bidirectional: True
use_adam: True
use_parallel: False
save_path: 'saved_models'
# dataset can be "conala" or "codesearchnet or cs105"
dataset: 'conala'
# encoder can be 'LSTM' or 'Transformer'
encoder: 'LSTM'
full_bimpm: True
save_every: 20
negative_examples: 5
# Direction of Translation: code2lang or lang2code
translate_task: 'code2lang'
tune_thres: True